{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db397e92",
   "metadata": {},
   "source": [
    "### RAG Pipeline complete from data ingestion to vector db\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e4256eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_community.document_loaders import PyMuPDFLoader , PyPDFLoader , TextLoader , DirectoryLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "68c3e11a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6 PDF files to process\n",
      "\n",
      "Processing: air pollution act.pdf\n",
      "  ✓ Loaded 43 pages\n",
      "\n",
      "Processing: ai_ml_rag_guide.pdf\n",
      "  ✓ Loaded 4 pages\n",
      "\n",
      "Processing: company_policies_faq.pdf\n",
      "  ✓ Loaded 4 pages\n",
      "\n",
      "Processing: python_features.pdf\n",
      "  ✓ Loaded 1 pages\n",
      "\n",
      "Processing: python_use_cases.pdf\n",
      "  ✓ Loaded 1 pages\n",
      "\n",
      "Processing: software_engineering_handbook.pdf\n",
      "  ✓ Loaded 3 pages\n",
      "\n",
      "Total documents loaded: 56\n"
     ]
    }
   ],
   "source": [
    "### Read all the pdf's inside the directory\n",
    "def process_all_pdfs(pdf_directory):\n",
    "    \"\"\"Process all PDF files in a directory\"\"\"\n",
    "    all_documents = []\n",
    "    pdf_dir = Path(pdf_directory)\n",
    "    \n",
    "    # Find all PDF files recursively\n",
    "    pdf_files = list(pdf_dir.glob(\"**/*.pdf\"))\n",
    "    \n",
    "    print(f\"Found {len(pdf_files)} PDF files to process\")\n",
    "    \n",
    "    for pdf_file in pdf_files:\n",
    "        print(f\"\\nProcessing: {pdf_file.name}\")\n",
    "        try:\n",
    "            loader = PyPDFLoader(str(pdf_file))\n",
    "            documents = loader.load()\n",
    "            \n",
    "            # Add source information to metadata\n",
    "            for doc in documents:\n",
    "                doc.metadata['source_file'] = pdf_file.name\n",
    "                doc.metadata['file_type'] = 'pdf'\n",
    "            \n",
    "            all_documents.extend(documents)\n",
    "            print(f\"  ✓ Loaded {len(documents)} pages\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  ✗ Error: {e}\")\n",
    "    \n",
    "    print(f\"\\nTotal documents loaded: {len(all_documents)}\")\n",
    "    return all_documents\n",
    "\n",
    "# Process all PDFs in the data directory\n",
    "all_pdf_documents = process_all_pdfs(\"../data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c96440b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Text splitting get into chunks\n",
    "\n",
    "def split_documents(documents,chunk_size=1000,chunk_overlap=200):\n",
    "    \"\"\"Split documents into smaller chunks for better RAG performance\"\"\"\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        length_function=len,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    "    )\n",
    "    split_docs = text_splitter.split_documents(documents)\n",
    "    print(f\"Split {len(documents)} documents into {len(split_docs)} chunks\")\n",
    "    \n",
    "    # example of a chunk\n",
    "    if split_docs:\n",
    "        print(f\"\\nExample chunk:\")\n",
    "        print(f\"Content: {split_docs[0].page_content[:200]}...\")\n",
    "        print(f\"Metadata: {split_docs[0].metadata}\")\n",
    "    \n",
    "    return split_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "55b503de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split 56 documents into 15 chunks\n",
      "\n",
      "Example chunk:\n",
      "Content: AIR (Prevention and control of Pollution) ACT...\n",
      "Metadata: {'producer': 'PyPDF', 'creator': 'Google', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\air pollution act.pdf', 'total_pages': 43, 'page': 0, 'page_label': '1', 'source_file': 'air pollution act.pdf', 'file_type': 'pdf'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'PyPDF', 'creator': 'Google', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\air pollution act.pdf', 'total_pages': 43, 'page': 0, 'page_label': '1', 'source_file': 'air pollution act.pdf', 'file_type': 'pdf'}, page_content='AIR (Prevention and control of Pollution) ACT'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Google', 'creationdate': '', 'source': '..\\\\data\\\\pdf\\\\air pollution act.pdf', 'total_pages': 43, 'page': 42, 'page_label': '43', 'source_file': 'air pollution act.pdf', 'file_type': 'pdf'}, page_content='THANK YOU'),\n",
       " Document(metadata={'producer': 'ReportLab PDF Library - www.reportlab.com', 'creator': '(unspecified)', 'creationdate': '2026-01-30T14:31:18+00:00', 'author': '(anonymous)', 'keywords': '', 'moddate': '2026-01-30T14:31:18+00:00', 'subject': '(unspecified)', 'title': '(anonymous)', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\ai_ml_rag_guide.pdf', 'total_pages': 4, 'page': 0, 'page_label': '1', 'source_file': 'ai_ml_rag_guide.pdf', 'file_type': 'pdf'}, page_content='Introduction to Artificial Intelligence\\nArtificial Intelligence refers to machines that can perform tasks requiring human intelligence.\\nExamples include natural language processing, computer vision, and decision-making systems.'),\n",
       " Document(metadata={'producer': 'ReportLab PDF Library - www.reportlab.com', 'creator': '(unspecified)', 'creationdate': '2026-01-30T14:31:18+00:00', 'author': '(anonymous)', 'keywords': '', 'moddate': '2026-01-30T14:31:18+00:00', 'subject': '(unspecified)', 'title': '(anonymous)', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\ai_ml_rag_guide.pdf', 'total_pages': 4, 'page': 1, 'page_label': '2', 'source_file': 'ai_ml_rag_guide.pdf', 'file_type': 'pdf'}, page_content='Machine Learning Basics\\nMachine learning enables systems to learn patterns from data.\\nSupervised, unsupervised, and reinforcement learning are common paradigms.\\nModel evaluation is performed using metrics like accuracy, precision, and recall.'),\n",
       " Document(metadata={'producer': 'ReportLab PDF Library - www.reportlab.com', 'creator': '(unspecified)', 'creationdate': '2026-01-30T14:31:18+00:00', 'author': '(anonymous)', 'keywords': '', 'moddate': '2026-01-30T14:31:18+00:00', 'subject': '(unspecified)', 'title': '(anonymous)', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\ai_ml_rag_guide.pdf', 'total_pages': 4, 'page': 2, 'page_label': '3', 'source_file': 'ai_ml_rag_guide.pdf', 'file_type': 'pdf'}, page_content='Embeddings and Vector Search\\nEmbeddings are numerical representations of text or data.\\nThey allow semantic similarity comparisons using vector distance.\\nVector databases like FAISS or Chroma are commonly used.'),\n",
       " Document(metadata={'producer': 'ReportLab PDF Library - www.reportlab.com', 'creator': '(unspecified)', 'creationdate': '2026-01-30T14:31:18+00:00', 'author': '(anonymous)', 'keywords': '', 'moddate': '2026-01-30T14:31:18+00:00', 'subject': '(unspecified)', 'title': '(anonymous)', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\ai_ml_rag_guide.pdf', 'total_pages': 4, 'page': 3, 'page_label': '4', 'source_file': 'ai_ml_rag_guide.pdf', 'file_type': 'pdf'}, page_content='Retrieval Augmented Generation (RAG)\\nRAG combines information retrieval with language models.\\nRelevant documents are retrieved and passed to the model as context.\\nThis improves factual accuracy and reduces hallucinations.'),\n",
       " Document(metadata={'producer': 'ReportLab PDF Library - www.reportlab.com', 'creator': '(unspecified)', 'creationdate': '2026-01-30T14:31:18+00:00', 'author': '(anonymous)', 'keywords': '', 'moddate': '2026-01-30T14:31:18+00:00', 'subject': '(unspecified)', 'title': '(anonymous)', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\company_policies_faq.pdf', 'total_pages': 4, 'page': 0, 'page_label': '1', 'source_file': 'company_policies_faq.pdf', 'file_type': 'pdf'}, page_content='Company Overview\\nThis document outlines internal company policies and guidelines.\\nIt is intended for employees and contractors.'),\n",
       " Document(metadata={'producer': 'ReportLab PDF Library - www.reportlab.com', 'creator': '(unspecified)', 'creationdate': '2026-01-30T14:31:18+00:00', 'author': '(anonymous)', 'keywords': '', 'moddate': '2026-01-30T14:31:18+00:00', 'subject': '(unspecified)', 'title': '(anonymous)', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\company_policies_faq.pdf', 'total_pages': 4, 'page': 1, 'page_label': '2', 'source_file': 'company_policies_faq.pdf', 'file_type': 'pdf'}, page_content='Work From Home Policy\\nEmployees may work from home up to three days per week.\\nApproval from the reporting manager is required.\\nProductivity and availability during work hours are expected.'),\n",
       " Document(metadata={'producer': 'ReportLab PDF Library - www.reportlab.com', 'creator': '(unspecified)', 'creationdate': '2026-01-30T14:31:18+00:00', 'author': '(anonymous)', 'keywords': '', 'moddate': '2026-01-30T14:31:18+00:00', 'subject': '(unspecified)', 'title': '(anonymous)', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\company_policies_faq.pdf', 'total_pages': 4, 'page': 2, 'page_label': '3', 'source_file': 'company_policies_faq.pdf', 'file_type': 'pdf'}, page_content='Leave and Holidays\\nEmployees are entitled to annual paid leave.\\nPublic holidays are observed as per the regional calendar.\\nUnplanned leaves should be communicated in advance where possible.'),\n",
       " Document(metadata={'producer': 'ReportLab PDF Library - www.reportlab.com', 'creator': '(unspecified)', 'creationdate': '2026-01-30T14:31:18+00:00', 'author': '(anonymous)', 'keywords': '', 'moddate': '2026-01-30T14:31:18+00:00', 'subject': '(unspecified)', 'title': '(anonymous)', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\company_policies_faq.pdf', 'total_pages': 4, 'page': 3, 'page_label': '4', 'source_file': 'company_policies_faq.pdf', 'file_type': 'pdf'}, page_content='IT and Security Guidelines\\nCompany devices must be secured with strong passwords.\\nConfidential data should not be shared externally.\\nVPN usage is mandatory when accessing internal systems remotely.'),\n",
       " Document(metadata={'producer': 'ReportLab PDF Library - www.reportlab.com', 'creator': '(unspecified)', 'creationdate': '2025-12-22T14:02:32+00:00', 'author': '(anonymous)', 'keywords': '', 'moddate': '2025-12-22T14:02:32+00:00', 'subject': '(unspecified)', 'title': '(anonymous)', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\python_features.pdf', 'total_pages': 1, 'page': 0, 'page_label': '1', 'source_file': 'python_features.pdf', 'file_type': 'pdf'}, page_content='Key Features of Python\\nPython is a high-level, interpreted programming language known for its simplicity and readability.\\nIt uses dynamic typing, which allows faster development without explicit type declarations.\\nPython supports multiple programming paradigms such as procedural, object-oriented, and\\nfunctional programming.\\nIt has a rich standard library that helps developers build applications efficiently.'),\n",
       " Document(metadata={'producer': 'ReportLab PDF Library - www.reportlab.com', 'creator': '(unspecified)', 'creationdate': '2025-12-22T14:02:32+00:00', 'author': '(anonymous)', 'keywords': '', 'moddate': '2025-12-22T14:02:32+00:00', 'subject': '(unspecified)', 'title': '(anonymous)', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\python_use_cases.pdf', 'total_pages': 1, 'page': 0, 'page_label': '1', 'source_file': 'python_use_cases.pdf', 'file_type': 'pdf'}, page_content='Use Cases of Python\\nPython is widely used in web development using frameworks like Django and Flask.\\nIt is a popular choice for data science, machine learning, and artificial intelligence.\\nPython is commonly used for automation and scripting to reduce repetitive tasks.\\nBecause of its simplicity, Python is widely used in education and rapid prototyping.'),\n",
       " Document(metadata={'producer': 'ReportLab PDF Library - www.reportlab.com', 'creator': '(unspecified)', 'creationdate': '2026-01-30T14:31:17+00:00', 'author': '(anonymous)', 'keywords': '', 'moddate': '2026-01-30T14:31:17+00:00', 'subject': '(unspecified)', 'title': '(anonymous)', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\software_engineering_handbook.pdf', 'total_pages': 3, 'page': 0, 'page_label': '1', 'source_file': 'software_engineering_handbook.pdf', 'file_type': 'pdf'}, page_content='Introduction to Software Engineering\\nSoftware engineering is the systematic application of engineering approaches to the development\\nof software.\\nIt involves requirement analysis, design, implementation, testing, deployment, and maintenance.\\nModern software engineering emphasizes scalability, maintainability, and collaboration.'),\n",
       " Document(metadata={'producer': 'ReportLab PDF Library - www.reportlab.com', 'creator': '(unspecified)', 'creationdate': '2026-01-30T14:31:17+00:00', 'author': '(anonymous)', 'keywords': '', 'moddate': '2026-01-30T14:31:17+00:00', 'subject': '(unspecified)', 'title': '(anonymous)', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\software_engineering_handbook.pdf', 'total_pages': 3, 'page': 1, 'page_label': '2', 'source_file': 'software_engineering_handbook.pdf', 'file_type': 'pdf'}, page_content='Frontend vs Backend Development\\nFrontend development focuses on user interfaces and user experience.\\nBackend development handles databases, APIs, authentication, and business logic.\\nFull-stack developers work across both frontend and backend layers.'),\n",
       " Document(metadata={'producer': 'ReportLab PDF Library - www.reportlab.com', 'creator': '(unspecified)', 'creationdate': '2026-01-30T14:31:17+00:00', 'author': '(anonymous)', 'keywords': '', 'moddate': '2026-01-30T14:31:17+00:00', 'subject': '(unspecified)', 'title': '(anonymous)', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\software_engineering_handbook.pdf', 'total_pages': 3, 'page': 2, 'page_label': '3', 'source_file': 'software_engineering_handbook.pdf', 'file_type': 'pdf'}, page_content='Version Control and Git\\nVersion control systems track changes in code over time.\\nGit is the most widely used distributed version control system.\\nCommon Git workflows include feature branching and pull requests.')]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks = split_documents(all_pdf_documents)\n",
    "chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d51f5a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "### embeddings and vector store db\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import chromadb\n",
    "import uuid\n",
    "from typing import List, Dict,Any,Tuple\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "efce30c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embedding model: all-MiniLM-L6-v2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|██████████| 103/103 [00:00<00:00, 359.88it/s, Materializing param=pooler.dense.weight]                             \n",
      "BertModel LOAD REPORT from: sentence-transformers/all-MiniLM-L6-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully. Embedding dimension: 384\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.EmbeddingManager at 0x19da09e7e00>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class EmbeddingManager:\n",
    "    \"\"\"Handles document embedding generation using SentenceTransformer\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str = \"all-MiniLM-L6-v2\"):\n",
    "        \"\"\"\n",
    "        Initialize the embedding manager\n",
    "        \n",
    "        Args:\n",
    "            model_name: HuggingFace model name for sentence embeddings\n",
    "        \"\"\"\n",
    "        self.model_name = model_name\n",
    "        self.model = None\n",
    "        self._load_model()\n",
    "\n",
    "    def _load_model(self):\n",
    "        \"\"\"Load the SentenceTransformer model\"\"\"\n",
    "        try:\n",
    "            print(f\"Loading embedding model: {self.model_name}\")\n",
    "            self.model = SentenceTransformer(self.model_name)\n",
    "            print(f\"Model loaded successfully. Embedding dimension: {self.model.get_sentence_embedding_dimension()}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading model {self.model_name}: {e}\")\n",
    "            raise\n",
    "\n",
    "    def generate_embeddings(self, texts: List[str]) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Generate embeddings for a list of texts\n",
    "        \n",
    "        Args:\n",
    "            texts: List of text strings to embed\n",
    "            \n",
    "        Returns:\n",
    "            numpy array of embeddings with shape (len(texts), embedding_dim)\n",
    "        \"\"\"\n",
    "        if not self.model:\n",
    "            raise ValueError(\"Model not loaded\")\n",
    "        \n",
    "        print(f\"Generating embeddings for {len(texts)} texts...\")\n",
    "        embeddings = self.model.encode(texts, show_progress_bar=True)\n",
    "        print(f\"Generated embeddings with shape: {embeddings.shape}\")\n",
    "        return embeddings\n",
    "\n",
    "\n",
    "## initialize the embedding manager\n",
    "\n",
    "embedding_manager=EmbeddingManager()\n",
    "embedding_manager\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf1f1cc",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "680c24c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector store initialized. Collection: pdf_documents\n",
      "Existing documents in collection: 4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.VectorStore at 0x19da0f87b60>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class VectorStore:\n",
    "    \"\"\"Manages document embeddings in a ChromaDB vector store\"\"\"\n",
    "    \n",
    "    def __init__(self, collection_name: str = \"pdf_documents\", persist_directory: str = \"../data/vector_store\"):\n",
    "        \"\"\"\n",
    "        Initialize the vector store\n",
    "        \n",
    "        Args:\n",
    "            collection_name: Name of the ChromaDB collection\n",
    "            persist_directory: Directory to persist the vector store\n",
    "        \"\"\"\n",
    "        self.collection_name = collection_name\n",
    "        self.persist_directory = persist_directory\n",
    "        self.client = None\n",
    "        self.collection = None\n",
    "        self._initialize_store()\n",
    "\n",
    "    def _initialize_store(self):\n",
    "        \"\"\"Initialize ChromaDB client and collection\"\"\"\n",
    "        try:\n",
    "            # Create persistent ChromaDB client\n",
    "            os.makedirs(self.persist_directory, exist_ok=True)\n",
    "            self.client = chromadb.PersistentClient(path=self.persist_directory)\n",
    "            \n",
    "            # Get or create collection\n",
    "            self.collection = self.client.get_or_create_collection(\n",
    "                name=self.collection_name,\n",
    "                metadata={\"description\": \"PDF document embeddings for RAG\"}\n",
    "            )\n",
    "            print(f\"Vector store initialized. Collection: {self.collection_name}\")\n",
    "            print(f\"Existing documents in collection: {self.collection.count()}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error initializing vector store: {e}\")\n",
    "            raise\n",
    "\n",
    "    def add_documents(self, documents: List[Any], embeddings: np.ndarray):\n",
    "        \"\"\"\n",
    "        Add documents and their embeddings to the vector store\n",
    "        \n",
    "        Args:\n",
    "            documents: List of LangChain documents\n",
    "            embeddings: Corresponding embeddings for the documents\n",
    "        \"\"\"\n",
    "        if len(documents) != len(embeddings):\n",
    "            raise ValueError(\"Number of documents must match number of embeddings\")\n",
    "        \n",
    "        print(f\"Adding {len(documents)} documents to vector store...\")\n",
    "        \n",
    "        # Prepare data for ChromaDB\n",
    "        ids = []\n",
    "        metadatas = []\n",
    "        documents_text = []\n",
    "        embeddings_list = []\n",
    "        \n",
    "        for i, (doc, embedding) in enumerate(zip(documents, embeddings)):\n",
    "            # Generate unique ID\n",
    "            doc_id = f\"doc_{uuid.uuid4().hex[:8]}_{i}\"\n",
    "            ids.append(doc_id)\n",
    "            \n",
    "            # Prepare metadata\n",
    "            metadata = dict(doc.metadata)\n",
    "            metadata['doc_index'] = i\n",
    "            metadata['content_length'] = len(doc.page_content)\n",
    "            metadatas.append(metadata)\n",
    "            \n",
    "            # Document content\n",
    "            documents_text.append(doc.page_content)\n",
    "            \n",
    "            # Embedding\n",
    "            embeddings_list.append(embedding.tolist())\n",
    "        \n",
    "        # Add to collection\n",
    "        try:\n",
    "            self.collection.add(\n",
    "                ids=ids,\n",
    "                embeddings=embeddings_list,\n",
    "                metadatas=metadatas,\n",
    "                documents=documents_text\n",
    "            )\n",
    "            print(f\"Successfully added {len(documents)} documents to vector store\")\n",
    "            print(f\"Total documents in collection: {self.collection.count()}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error adding documents to vector store: {e}\")\n",
    "            raise\n",
    "\n",
    "vectorstore=VectorStore()\n",
    "vectorstore\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a680dc26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embeddings for 15 texts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  1.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embeddings with shape: (15, 384)\n",
      "Adding 15 documents to vector store...\n",
      "Successfully added 15 documents to vector store\n",
      "Total documents in collection: 19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "### Convert the text to embeddings\n",
    "texts=[doc.page_content for doc in chunks]\n",
    "\n",
    "## Generate the Embeddings\n",
    "\n",
    "embeddings=embedding_manager.generate_embeddings(texts)\n",
    "\n",
    "##store in the vector dtaabase\n",
    "vectorstore.add_documents(chunks,embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6b78c4ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAGRetriever:\n",
    "    \"\"\"Handles query-based retrieval from the vector store\"\"\"\n",
    "    \n",
    "    def __init__(self, vector_store: VectorStore, embedding_manager: EmbeddingManager):\n",
    "        \"\"\"\n",
    "        Initialize the retriever\n",
    "        \n",
    "        Args:\n",
    "            vector_store: Vector store containing document embeddings\n",
    "            embedding_manager: Manager for generating query embeddings\n",
    "        \"\"\"\n",
    "        self.vector_store = vector_store\n",
    "        self.embedding_manager = embedding_manager\n",
    "\n",
    "    def retrieve(self, query: str, top_k: int = 5, score_threshold: float = 0.0) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Retrieve relevant documents for a query\n",
    "        \n",
    "        Args:\n",
    "            query: The search query\n",
    "            top_k: Number of top results to return\n",
    "            score_threshold: Minimum similarity score threshold\n",
    "            \n",
    "        Returns:\n",
    "            List of dictionaries containing retrieved documents and metadata\n",
    "        \"\"\"\n",
    "        print(f\"Retrieving documents for query: '{query}'\")\n",
    "        print(f\"Top K: {top_k}, Score threshold: {score_threshold}\")\n",
    "        \n",
    "        # Generate query embedding\n",
    "        query_embedding = self.embedding_manager.generate_embeddings([query])[0]\n",
    "        \n",
    "        # Search in vector store\n",
    "        try:\n",
    "            results = self.vector_store.collection.query(\n",
    "                query_embeddings=[query_embedding.tolist()],\n",
    "                n_results=top_k\n",
    "            )\n",
    "            \n",
    "            # Process results\n",
    "            retrieved_docs = []\n",
    "            \n",
    "            if results['documents'] and results['documents'][0]:\n",
    "                documents = results['documents'][0]\n",
    "                metadatas = results['metadatas'][0]\n",
    "                distances = results['distances'][0]\n",
    "                ids = results['ids'][0]\n",
    "                \n",
    "                for i, (doc_id, document, metadata, distance) in enumerate(zip(ids, documents, metadatas, distances)):\n",
    "                    # Convert distance to similarity score (ChromaDB uses cosine distance)\n",
    "                    similarity_score = 1 - distance\n",
    "                    \n",
    "                    if similarity_score >= score_threshold:\n",
    "                        retrieved_docs.append({\n",
    "                            'id': doc_id,\n",
    "                            'content': document,\n",
    "                            'metadata': metadata,\n",
    "                            'similarity_score': similarity_score,\n",
    "                            'distance': distance,\n",
    "                            'rank': i + 1\n",
    "                        })\n",
    "                \n",
    "                print(f\"Retrieved {len(retrieved_docs)} documents (after filtering)\")\n",
    "            else:\n",
    "                print(\"No documents found\")\n",
    "            \n",
    "            return retrieved_docs\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error during retrieval: {e}\")\n",
    "            return []\n",
    "\n",
    "rag_retriever=RAGRetriever(vectorstore,embedding_manager)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c8626053",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.RAGRetriever at 0x19da0f87cb0>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7bf743d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving documents for query: 'This document outlines internal company policies and guidelines.'\n",
      "Top K: 5, Score threshold: 0.0\n",
      "Generating embeddings for 1 texts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 37.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embeddings with shape: (1, 384)\n",
      "Retrieved 1 documents (after filtering)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'id': 'doc_e0c407b2_6',\n",
       "  'content': 'Company Overview\\nThis document outlines internal company policies and guidelines.\\nIt is intended for employees and contractors.',\n",
       "  'metadata': {'source': '..\\\\data\\\\pdf\\\\company_policies_faq.pdf',\n",
       "   'content_length': 127,\n",
       "   'keywords': '',\n",
       "   'title': '(anonymous)',\n",
       "   'total_pages': 4,\n",
       "   'creationdate': '2026-01-30T14:31:18+00:00',\n",
       "   'subject': '(unspecified)',\n",
       "   'producer': 'ReportLab PDF Library - www.reportlab.com',\n",
       "   'doc_index': 6,\n",
       "   'page_label': '1',\n",
       "   'file_type': 'pdf',\n",
       "   'author': '(anonymous)',\n",
       "   'creator': '(unspecified)',\n",
       "   'trapped': '/False',\n",
       "   'moddate': '2026-01-30T14:31:18+00:00',\n",
       "   'source_file': 'company_policies_faq.pdf',\n",
       "   'page': 0},\n",
       "  'similarity_score': 0.8484464287757874,\n",
       "  'distance': 0.15155357122421265,\n",
       "  'rank': 1}]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_retriever.retrieve(\"This document outlines internal company policies and guidelines.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a206459f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RAGdemo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
